---
title: "GANomaly: Semi-supervised Anomaly Detection via Adversarial Training"
date: 2021-08-19
identifier: GANomaly
category: "Anomaly Detection"
hero: /assets/images/posts/GANomaly/overview.png
link: https://arxiv.org/abs/1805.06725
tags: ["Anomaly Detection", "Unsupervised Learning", "GAN"]
conference: ACCV
year: 2018
math: true
layout: post
---

# 1. どんなもの?
<!-- 概要・貢献等 100-200字程度 -->
* オートエンコーダとGANを組み合わせた異常検知
* ジェネレータをオートエンコーダのような構造にし、生成画像に判別機とは別にエンコーダをつけた
<!--more-->

# 2. 先行研究と比べてどこがすごい？
<!-- related worksとの差分 -->
* あらゆる異常検出タスクに一般化できる能力を持ち，最新のGANベースの異常検出アプローチと従来のオートエンコーダベースの異常検出アプローチの両方を上回ることがわかった

# 3. 技術や手法の"キモ"はどこ？
<!-- キモを箇条書きでまとめる -->

## 変数定義
<!--
学習・推論で使う変数をまとめる
* $x$: 入力画像
* $y$: 教師信号
-->
* $x$: 元画像
* $\hat{x}$: 生成画像
* $G(\cdot)$: 生成器
* $E(\cdot)$: エンコーダ
* $G_E(\cdot)$: 生成器のエンコーダ
* $f(\cdot)$: 判別機の中間層出力
* $w_{adv}, w_{con}, w_{enc} $: 調整用重み付けパラメータ

## 学習
<!-- キモの中の学習に関する内容 -->
* ネットワーク構造
    * ![](/assets/images/posts/GANomaly/overview.png)

* 目的関数
    * $$ \mathcal{L} = w_{adv} \mathcal{L}_{adv} + w_{con} \mathcal{L}_{con} + w_{enc} \mathcal{L}_{enc} $$
    * Adversarial Loss
        * $$ \mathcal{L}_{adv} = \mathbb{E}_{x \sim pX} \| f(x) - \mathbb{E}_{x \sim pX} f\big(G(x) \big) \|_2 $$
        * 元画像と生成画像を識別機に入れた中間層のL2距離
    * Contextual Loss
        * $$ \mathcal{L}_{con} = \mathbb{E}_{x \sim pX} \| x - G(x) \|_1 $$
        * 元画像と生成画像のL1距離
            * L1はL2よりぼやけないため
    * Encoder Loss
        * $$ \mathcal{L}_{enc} = \mathbb{E}_{x \sim pX} \| G_E(x) - E(G(x)) \|_2 $$
        * 元画像のボトルネック特徴と生成画像の潜在特徴のL2距離

## 推論（異常度の算出）
<!-- キモの中の推論に関する内容 -->
* 推論時は学習時のEncoder Lossを使用
    * $$ \mathcal{A}(\hat{x}) = \| G_E ( \hat{x} ) - E (G( \hat{x})) \|\_1 $$

# 4. どうやって有効だと検証した？
<!-- 実験の精度，結果画像など -->
* MNISTとCIFAR10
* ![](/assets/images/posts/GANomaly/MNIST.png)

* UBAとFFOB
    * X線セキュリティ検査
    * ![](/assets/images/posts/GANomaly/UBA.png)
