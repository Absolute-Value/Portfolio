---
title: "QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information"
date: "2022-10-20 12:00:00"
identifier: QPIC
category: HOI
hero: /assets/images/posts/QPIC/1.png
link: https://arxiv.org/abs/2103.05399
tags: ["Human-Object Interaction Detection", "Transformer"]
conference: CVPR
year: 2021
math: true
layout: post
---

# 概要

- CNNベースのHOI手法ではCNNの局所性により全体の特徴を使用できず、手動で設定した関心領域に依存し、複数のHOIを混在する欠点がある
- transformerベースの特徴抽出器を利用することで、画像全体を集約し複数のHOIの混在を避けることができる
- 効果的なtransformerベースの特徴抽出器によって検出ヘッドがシンプルで直感的になり、文脈上重要な特徴をうまく抽出し、既存手法を大きく上回った
    - HICO-DETで5.37mAP↑,V-COCOで5.7mAP↑
<!--more-->

# 新規性・差分

- transformerを使用し、文脈特徴をペアワイズで集約

# アイデア

![](/assets/images/posts/QPIC/1.png)

- アーキテクチャ
    - 事前学習済みCNNで特徴マップを取得し、畳み込みでチャンネル数を調整
    - Positional Encodingを追加し、Transformer encoder, decoderに通す
        - decoderの入力(Query vectors)は学習
        - Query vectorsの数はインタラクション数
    - Interaction detection heads
        - Human box FFN
            - 人間のバウンディングボックスを予測
        - Object box FFN
            - 物体のバウンディングボックスを予測
        - Object class FFN
            - 物体のクラスを予測
        - Action class FFN
            - 行動のクラスを予測
- Loss
    - 予測と正解のに分割マッチング
        - DETRのHungarianアルゴリズム
    - マッチングされたペアに対するlossの計算
        - $L = \lambda_b L_b + \lambda_u L_u + \lambda_c L_c + \lambda_a L_a$
            - $L_b$：人と物体のバウンディングボックスの座標のLoss
                - ![](/assets/images/posts/QPIC/2.png)
            - $L_u$：人と物体のバウンディングボックスのIoUのLoss
                - ![](/assets/images/posts/QPIC/3.png)
            - $L_c$：物体のクラス分類Loss
                - ![](/assets/images/posts/QPIC/4.png)
            - $L_a$：行動のクラス分類Loss
                - ![](/assets/images/posts/QPIC/5.png)

# 結果

- HICO-DET
    - 5.37mAP↑
        - ![](/assets/images/posts/QPIC/6.png)
        
- V-COCO
    - 5.7mAP↑
        - ![](/assets/images/posts/QPIC/7.png)
        
- 既存手法との比較
    - ![](/assets/images/posts/QPIC/8.png)
    - 既存手法
        - (a)(b)はDRF, (c)(d)はPPDM→行動の検出に失敗
    - QPIC
        - 下段のAttentionマップからわかるように物体を見て検出できた