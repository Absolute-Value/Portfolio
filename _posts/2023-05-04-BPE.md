---
title: "Neural Machine Translation of Rare Words with Subword Units"
date: "2023-05-04 12:00:00"
identifier: BPE
category: Language
hero: /assets/images/posts/BPE/0.png
link: https://arxiv.org/abs/1508.07909
tags: ["Language"]
conference: ACM
year: 2016
math: true
layout: post
---

# 概要

- 実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切）
- そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した
- これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上
<!--more-->

# 新規性・差分

- open-vocabularyでニューラル機械翻訳(NMT)

# アイデア

- アルゴリズム
    - ![](/assets/images/posts/BPE/1.png)
    - BPE
        - シーケンス内で最も頻繁に使用されるバイトのペアを、単一の未使用バイトに繰り返し置き換えていく
        - 繰り返す回数はハイパラ
    - 例
        - 「l o w </w>(5個), l o w e s t </w>(2個), n e w e r </w>(6個), w i d e r </w>(3個)」という辞書がある場合（4回繰り返す）
            - 最も頻繁に出てくるeとrの組み合わせを結合(er)
            - 最も頻繁に出てくるerと</w>の組み合わせを結合(er</w>)
            - 最も頻繁に出てくるlとoの組み合わせを結合(lo)
            - 最も頻繁に出てくるloとwの組み合わせを結合(low)
        - 「low </w>, low e s t </w>, n e w er</w>, w i d er</w>」という辞書になる
        - 12(l o w e s t n r w i d </w>)から10(low e s t n w er</w> i d </w>)に減った
    - ソースとターゲットで独立してEncodingを学習する方法と
    ソースとターゲットの結合でEncodingを学習する方法(joint BPE)を用意
        - 独立する場合はニューラルモデルがサブワード単位間の翻訳を学習するのが難しくなる

# 結果

- newest2015(英→独)
    - ![](/assets/images/posts/BPE/2.png)
    - rareは上位5万語に含まれない単語

- newest2015(英→露)
    - ![](/assets/images/posts/BPE/3.png)

- 単語の分割例(英→独)
    - ![](/assets/images/posts/BPE/4.png)
    
- 単語の分割例(英→露)
    - ![](/assets/images/posts/BPE/5.png)