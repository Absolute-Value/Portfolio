---
title: "BEIT: BERT Pre-Training of Image Transformers"
date: "2023-10-16 20:06:00"
identifier: MusicTransformer
category: Transformer
hero: /assets/images/posts/BEIT/0.png
link: https://aka.ms/beit
tags: ["Vision Pretraining", "Transformer"]
conference: ICLR
year: 2022
math: true
layout: post
---

# 概要

- Vision Transformer事前学習する自己教師ありタスクを提案
- BERTのようなマスク画像モデリングを行う
- 画像分類とセマンティックセグメンテーションで競争力のある結果を達成し、事前トレーニング方法を改善
<!--more-->

# 新規性・差分

- BERTスタイルの事前トレーニングを画像データに直接適用するのが難しいという課題を解決

# アイデア

- ![](/assets/images/posts/BEIT/0.png)
- 事前学習
    - 入力画像をdiscrete VAEでvisual tokensにする
    - 同時に入力画像をパッチ分割し、ランダムにマスクしてtransformerへ
    - transformerはマスクされたパッチに対応するvisual tokensを予測するように学習
- この事前学習をしたモデルをダウンストリームタスクに応用

# 結果

- 画像分類（ILSVRC-2012 ImageNet）
    - ![](/assets/images/posts/BEIT/1.png)
    
- セマンティックセグメンテーション
    - ![](/assets/images/posts/BEIT/2.png)