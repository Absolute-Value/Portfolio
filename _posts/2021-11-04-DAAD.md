---
title: "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection"
date: "2021-11-04 09:00:00"
identifier: DAAD
category: "Anomaly Detection"
hero: /assets/images/posts/DAAD/frame.png
link: https://arxiv.org/abs/2107.13118
tags: ["Anomaly Detection", "Unsupervised Learning", "GAN"]
conference: ICCV
year: 2021
math: true
layout: post
---

# 1. どんなもの?
<!-- 概要・貢献等 100-200字程度 -->
* UNet構造にメモリを取り入れ、敵対的学習も行う再構成ベース異常検知
<!--more-->

# 2. 先行研究と比べてどこがすごい？
<!-- related worksとの差分 -->
* AEではニューラルネットの汎化性を制御することが困難
* マルチスケール・ブロックワイズ・メモリー・モジュールによって、正常の良い再構成と異常の不十分な再構成のトレードオフを実現
* 敵対的学習を導入することで、微小な異常検知性能も上昇

# 3. 技術や手法の"キモ"はどこ？
<!-- キモを箇条書きでまとめる -->
![](/assets/images/posts/DAAD/frame.png)

* マルチスケール・ブロックワイズ・メモリー・モジュール
    * 正常なパターンの記憶に使用
    * 細かい仕組み
        * 特徴マップを$Q=r_h \times r_w \times r_c$に分割
            * クエリ $q_k$
        * 平滑化
        * アテンション重み $w$ を計算
            * $$ w_i = \frac{exp(\frac{q^k \quad m^T_i}{\| q^k \| \quad \| m_i \|})}{\sum^N_{j=1} exp(\frac{q^k \quad m^T_j}{\| q^k \| \quad \| m_j \|})} $$
        * $w$ を用いて、クエリ $q_k$ に最も関連するメモリアイテムを探索し対応する $\hat{q}^k$ を生成
            * $$ \hat{q}^k = wM = \sum^N_{i=1} w_im_i $$
        * 形を戻してDecoderへ

## 変数定義
<!--
学習・推論で使う変数をまとめる
* $x$: 入力画像
* $y$: 教師信号
-->

## 学習
<!-- キモの中の学習に関する内容 -->
* ネットワーク構造
    * ![](/assets/images/posts/DAAD/train.png)

* Loss関数
    * $$ \mathcal{L} = \lambda_{rec} \mathcal{L}_{rec} + \lambda_{adv} \mathcal{L}_{adv} + \lambda_{ali} \mathcal{L}_{ali} $$
    * 再構成Loss
        * $$ \mathcal{L}_{rec} = \mathbb{E}_{x \sim p_x} \| x - \hat{x} \|_2 $$
        * L2距離
    * 敵対的Loss
        * $$ \mathcal{L}_{adv} = \mathbb{E}_{x \sim p_x} [\log D(x)] + \mathbb{E}_{x \sim p_x} [\log (1-D(G(x)))] $$
        * 識別器の学習用に使用する、GANで提案されたLoss
    * アライメントLoss
        * $$ \mathcal{L}_{ali} = \mathbb{E}_{x \sim p_x} \| f_D(x) - f_D(\hat{x}) \|_2 $$
        * 識別機の中間特徴の一致

## 推論（異常度の算出）
<!-- キモの中の推論に関する内容 -->
* ネットワーク構造
    * ![](/assets/images/posts/DAAD/test.png)

* 異常度の算出
    * $$ A(x) = \gamma R(x) + (1-\gamma) L(x) $$
        * $R(x)$ は再構成Lossと同じ
        * $L(x)$ はアライメントLossと同じ

# 4. どうやって有効だと検証した？
<!-- 実験の精度，結果画像など -->
* MNIST / Cifar10\
    * ![](/assets/images/posts/DAAD/mnist.png)
* MVTec\
    * ![](/assets/images/posts/DAAD/mvtec.png)

# 5. 関連文献
<!--
1. D. P. Kingma and J. Ba: “Adam: A method for stochastic optimization,”arXiv preprint arXiv:1412.6980,(2014).
2. P. Isola,J. Y. Zhu,T. Zhou,and A. A. Efros: “Image-to-image translation with conditional adversarial networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, (2017), 1125.
-->
1. Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Anton
van den Hengel. Memorizing normality to detect anomaly:
Memory-augmented deep autoencoder for unsupervised
anomaly detection. In Proceedings of the IEEE International
Conference on Computer Vision, pages 1705–1714, 2019. 2,