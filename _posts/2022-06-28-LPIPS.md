---
title: "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"
date: 2022-06-28
identifier: LPIPS
category: "Perceptual Metrics"
hero: /assets/images/posts/LPIPS/1.png
link: https://github.com/richzhang/PerceptualSimilarity
tags: ["Perceptual Metrics"]
conference: CVPR
year: 2018
math: true
layout: post
---

## 概要

- 知覚的な類似性としてPSNRやSSIMが広く使われているが、浅い関数のため人間の知覚の多くのニュアンスを説明することができない
- そこで、学習済みモデルの特徴を使用
- 教師あり、自己教師あり、教師なしを問わずに学習済みモデルの特徴がこれまでの指標を凌駕し、人間の知覚的判断に対応していることを確認
<!--more-->

## アイデア

- 既存手法の特徴量を用いて、どちらのパッチが元の画像に近いかを答える
    - 教師なしのK-means
    - 自己教師ありのBiGANなど
    - 教師ありのAlexNetなど
- LPIPS (Learned Perceptual Image Patch Similarity)  
    - 一般的に使われるVGG、人間の視覚野に近いAlexNet、AlexNetと同等の性能ながら軽量であるSqueezeNetの3つのネットワークを使用
    - 比較したいパッチごとにネットワークに通し、特徴量から距離を算出
    - 算出した二つの距離から、知覚的判断を小さなネットワークで算出

![](/assets/images/posts/LPIPS/2.png)

## 結果

- どちらが元のパッチに近いかを答える問題

![](/assets/images/posts/LPIPS/3.png)

- 分類と検出性能の高い特徴ほど、知覚的類似性を判断するモデルとしての性能が高い