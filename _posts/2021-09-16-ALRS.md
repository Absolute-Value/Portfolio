---
title: "Automated Learning Rate Scheduler for Large-batch Training"
date: 2021-09-16
identifier: ALRS
category: "Anomaly Detection"
hero: /assets/images/posts/ALRS/lr.png
link: https://arxiv.org/abs/2107.05855
tags: ["Anomaly Detection"]
conference: ICML
year: 2021
math: true
layout: post
---

## 1. どんなもの?
<!-- 概要・貢献等 100-200字程度 -->
* 大規模なデータやモデルを使用する際に計算効率からバッチサイズを大きくする
* しかし、バッチサイズを大きくすると小さいものと同等の性能を出すのに特別設計の学習率が必要となることがある
* そこで、train_lossが減少しなくなるまで学習率を増加させ、終了までに０に減少させるというアルゴリズムを開発
* バッチサイズでチューニング済みのベースラインと同等以上の性能を達成
<!--more-->

## 2. 先行研究と比べてどこがすごい？
<!-- related worksとの差分 -->
* バッチサイズに合わせた学習率のチューニングが必要なくなる

## 3. 技術や手法の"キモ"はどこ？
<!-- キモを箇条書きでまとめる -->
![](/assets/images/posts/ALRS/lr.png)

### 変数定義
<!--
学習・推論で使う変数をまとめる
* $x$: 入力画像
* $y$: 教師信号
-->
* $\theta$: 平均
* $K_{l,\sigma_f}$: 共分散カーネル
* $\sigma^2_n$: ノイズレベル

>>### ウォームアップ段階

* train_lossが減少しなくなるまで学習率を増加させる  

* Lossが減少する限り、$\rho_\omega T$ ステップの間 $\eta_t$ は$\eta_{max}$まで更新し続ける
![](/assets/images/posts/ALRS/func1.png)
    * 指数関数的な成長をする
        * 大規模バッチにおいて安定してきめ細かい学習率探索を可能にしている

* GP回帰を用いてLoss曲線を平滑化したものでLossが減少しているか判定する
![](/assets/images/posts/ALRS/2.png)
    * 局所的変動対策
* 下の数式で終了したいが、計算が簡単ではないので、
![](/assets/images/posts/ALRS/3.png)
* 下のような下界を計算する
![](/assets/images/posts/ALRS/4.png)

### 減衰段階

* 終了までに0に減衰させる
* 以下の二つでスケジュール
    * cosine減衰のみ
    * 途中まで一定の学習率で、最後の20%でcosine減衰

## 4. どうやって有効だと検証した？
<!-- 実験の精度，結果画像など -->

* 学習率とLoss
![](/assets/images/posts/ALRS/lr_loss.png)

## 5. 議論はあるか？
<!-- 自分なりの考察や疑問-->

>## 6. 関連文献
<!--
1. D. P. Kingma and J. Ba: “Adam: A method for stochastic optimization,”arXiv preprint arXiv:1412.6980,(2014).
2. P. Isola,J. Y. Zhu,T. Zhou,and A. A. Efros: “Image-to-image translation with conditional adversarial networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, (2017), 1125.
-->
1. Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan
Kim, Youngjung Uh, and Jung-Woo Ha. Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Iz3zU3M316D.
